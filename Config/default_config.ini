[general]
# Sets the name of the experiment for mlflow:
experiment: senet_fdhh_opt
# Seed for reproducibility
seed: 999
# Choose which data to use:
# Options: audio, video, combined
feature_type: video
# Audio features to use
# Options: AVEC, XCORR, EGEMAPS
audio_features: AVEC
# Video features to use (add FD at the end to use fdhh)
# Options: VGG_32_(FD) - 32nd fully connected layer from standard VGGFace architecture
#          RES_AVGPOOL_(FD) - last avgpool layer from RESNET architecture
#          SE_AVGPOOL_(FD) - last avgpool layer from SENET architecture
video_features: SE_AVGPOOL_FD

# Parameters for early/mid fusion:
[combined]
# Names for the models split by a (+) sign (e.g: PLS+LR):
# Options: LR, PLS, FNN
combined_model: PLS+LR
# Weights for the models, should add up to 1 (e.g: 0.5+0.5):
combined_model_weights: 0.7+0.3
# When to fuse the results
# Options: early - fuse at feature level and then perform scaling and dimensionality reduction
#          mid - preprocess features individually and then fuse two final feature vectors
#          late - make predictions using individual features and then fuse them at the end
fusion: early
# Define how  data should be scaled during preprocessing
# Options: standard - uses StandardScaler from sklearn
#          minmax - uses MinMaxScaler from sklearn
#          boxcox - optionally add with a (+) sign to transform long-tail distribution
combined_scaler: minmax
# Options: 0 - scale column-wise (used if features are not related)
#          1 - scale row-wise
#          None (default if left empty) - scale all features at once (used if feature represent same thing (e.g: pixels)
combined_scale_axis: 0
# value required for predictions over bimodal mode
# i.e. where (feature_type = combined) and (fusion = late)
prediction_weights:

[audio]
audio_model: PLS
audio_model_weights: 1
audio_scaler: boxcox+standard
audio_scale_axis: +0

[video]
video_model: LR
video_model_weights: 1
video_scaler: minmax
video_scale_axis: 0

[folders]
# Raw video file location:
raw_video_folder: C:\Darwin-Project\data\video\Video
# Face coordinate files:
facial_data: data\video\faces
# Extracted features location:
video_folder: data\video\features
# Raw audio file locations
raw_audio_folder: data\audio\raw
seg_audio_folder: data\audio\segments
audio_folder: data\audio\features
# Labels folder:
labels_folder: data\labels\AVEC2014_Labels
# Location of models:
models_folder: models


