[general]
# Sets the name of the experiment for mlflow:
experiment: baseline_combined
# Seed for reproducibility
seed: 999
# Choose which data to use:
# Options: audio, video, combined
feature_type: combined
# Audio features to use
# Options: AVEC    - all AVEC2014 baseline features extracted using OpenSmile
#          XCORR   - a series of cross correlated features as explained in Williamson's paper
#          EGEMAPS - features extracted using the egemaps config provided by OpenSmile
audio_features: AVEC
# Video features to use (add FD at the end to use fdhh)
# Options: VGG_32_(FD) - 32nd fully connected layer from standard VGGFace architecture
#          RES_AVGPOOL_(FD) - last avgpool layer from RESNET architecture
video_features: VGG_32_FD

# Parameters for early/mid fusion:
[combined]
# Names for the models split by a + sign (e.g: PLS+LR):
# Options: LR, PLS, FNN
combined_model: FNN+LR
# Weights for the models, should add up to 1 (e.g: 0.5+0.5):
combined_model_weights: 0.4+0.6
# When to fuse the results
# Options: early - fuse at feature level and then perform scaling and dimensionality reduction
#          mid - preprocess features individually and then fuse two final feature vectors
#          late - make predictions using individual features and then fuse them at the end
fusion: mid
# Define how  data should be scaled during preprocessing
# Options: standard - uses StandardScaler from sklearn
#          minmax - uses MinMaxScaler from sklearn
#          boxcox - optionally add with a (+) sign to transform long-tail distribution
combined_scaler: minmax
# Options: feature - scale feature-wise (used if features are not related)
#          full - scale all features at once (used if feature represent same thing (e.g: pixels)
combined_scale_over: feature
# value required for predictions over bimodal mode
# i.e. where (feature_type = combined) and (fusion = late)
prediction_weights: 0.7+0.3

[audio]
audio_model: PLS+LR
audio_model_weights: 0.5+0.5
audio_scaler: minmax
audio_scale_over: feature

[video]
video_model: PLS+LR
video_model_weights: 0.5+0.5
video_scaler: boxcox+minmax
video_scale_over: full+feature


[folders]
# Raw video file location:
raw_video_folder: D:\Education\University\Darwin-Project\data\video\Video
# Face coordinate files:
facial_data: data\video\faces
# Extracted features location:
video_folder: data\video\features
# Raw audio file locations
raw_audio_folder: data\audio\raw
seg_audio_folder: data\audio\segments
audio_folder: data\audio\features
# Labels folder:
labels_folder: data\labels\AVEC2014_Labels
# Location of models:
models_folder: models


